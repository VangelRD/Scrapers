# Comick.live Manhwa Scraper

A **ultra-high-performance** Go scraper for downloading manhwas from comick.live. Engineered for **maximum speed** with massive parallel processing and intelligent discovery algorithms.

## ðŸš€ Performance Highlights

- **Target: <1 hour** for full database downloads
- **Parallel Processing**: 100 pages + 10 manhwas + 5 chapters + 20 images simultaneously
- **25x faster** page discovery vs sequential processing
- **Smart Chapter Discovery**: Finds ALL chapters via paginated API support
- **Zero Waste**: Eliminated unnecessary API calls for 10x efficiency improvement

## âœ¨ Features

- **3 Scraping Modes**: Full download, specific slug, or download after a specific manhwa ID
- **Complete Chapter Discovery**: Automatically detects and downloads from paginated chapter lists
- **Cover Images**: Automatically downloads cover images for each manhwa
- **Massive Parallel Processing**: Concurrent processing at every level for maximum speed
- **Intelligent Termination**: Stops after 3 consecutive 404s to avoid server hammering
- **Multiple Groups Support**: Downloads same chapters from different scanlation groups
- **Rate Limiting**: Advanced rate limiting to respect server limits while maximizing speed
- **English Only**: Automatically filters for English chapters
- **Organized Storage**: Downloads are organized by manhwa slug and chapter number
- **Robust Error Handling**: Continues downloading even if individual chapters fail

## ðŸ“¦ Installation

1. Make sure you have Go installed (version 1.21 or higher)
2. Clone or download this repository
3. Navigate to the project directory
4. Build the scraper:

```bash
go build -o comick-scraper main.go
```

## ðŸŽ¯ Usage

The scraper supports three modes with **massive parallel processing**:

### 1. Full Download (Download All Manhwas)

Downloads all available manhwas and their chapters from comick.live:

```bash
./comick-scraper -mode=full
```

**Performance**: Completes in approximately **1 hour** thanks to parallel processing of 100 pages simultaneously.

### 2. Download Specific Manhwa by Slug

Download all chapters for a specific manhwa using its slug:

```bash
./comick-scraper -mode=slug -slug=the-mad-dog-of-the-duke-s-estate
```

To find a manhwa's slug, check its URL on comick.live. For example:
- URL: `https://comick.live/comic/the-mad-dog-of-the-duke-s-estate`
- Slug: `the-mad-dog-of-the-duke-s-estate`

### 3. Download Manhwas After Specific ID

Download all manhwas that have an ID greater than or equal to the specified ID:

```bash
./comick-scraper -mode=after-id -start-id=1000
```

This is useful for resuming downloads or only getting newer manhwas.

## âš™ï¸ Configuration Options

### Concurrent Workers

Control the number of concurrent download workers (default: 20, max: 50):

```bash
./comick-scraper -mode=slug -slug=your-manhwa -workers=30
```

**Optimization**: The scraper automatically uses:
- **100 parallel page fetchers** for manhwa discovery
- **10 parallel manhwa processors** 
- **5 parallel chapter processors** per manhwa
- **Your specified workers** for image downloads

## ðŸ“ Download Structure

Downloads are organized in the following structure:

```
downloads/
â”œâ”€â”€ manhwa-slug-1/
â”‚   â”œâ”€â”€ cover.webp          â† Cover image
â”‚   â”œâ”€â”€ chapter_1/
â”‚   â”‚   â”œâ”€â”€ 000.webp
â”‚   â”‚   â”œâ”€â”€ 001.webp
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ chapter_2/
â”‚   â”‚   â”œâ”€â”€ 000.webp
â”‚   â”‚   â”œâ”€â”€ 001.webp
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ manhwa-slug-2/
â”‚   â”œâ”€â”€ cover.webp
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## ðŸ“‹ Examples

### Download a single manhwa with high performance:
```bash
./comick-scraper -mode=slug -slug=solo-leveling -workers=30
```

### Download all manhwas with maximum speed:
```bash
./comick-scraper -mode=full -workers=50
```

### Resume downloading from manhwa ID 500:
```bash
./comick-scraper -mode=after-id -start-id=500 -workers=25
```

## ðŸ”§ Technical Details

The scraper uses **advanced parallel processing** and follows the optimized API pattern:

### Parallel Page Discovery
1. **Parallel Manhwa List Fetching**: Processes 100 pages of `/api/search?page=N` simultaneously
2. **Speed**: ~25 pages/second vs 1.5 pages/second sequential

### Smart Chapter Discovery
1. **Paginated Chapter Lists**: Uses `/api/comics/[SLUG]/chapter-list?page=N` when needed
2. **Complete Discovery**: Finds ALL chapters across multiple pages (e.g., 661 chapters vs 60)
3. **3 Consecutive 404s Logic**: Efficiently detects pagination end

### Optimized Image Extraction
1. **Direct HTML Parsing**: Eliminated wasteful API endpoint testing
2. **Pattern Matching**: Extracts image hashes from escaped JSON in HTML
3. **CDN Downloads**: `cdn1.comicknew.pictures/[SLUG]/0_[CHAPTER]/en/[HASH]/[N].webp`

### Parallel Processing Architecture
```
Page Discovery:    [100 concurrent pages]
    â†“
Manhwa Processing: [10 concurrent manhwas]
    â†“
Chapter Processing: [5 concurrent chapters per manhwa]
    â†“
Image Downloads:   [20+ concurrent images per chapter]
```

## âš¡ Performance & Rate Limiting

### Advanced Rate Limiting
- **Page Discovery**: 100 concurrent page fetchers
- **API Requests**: Up to 50 concurrent requests  
- **Manhwa Processing**: 10 parallel manhwas
- **Chapter Processing**: 50 parallel chapters globally
- **Image Downloads**: User-configurable (20-50 workers)

### Performance Metrics
- **Page Discovery**: 22+ pages/second
- **Chapter Discovery**: Finds 2x more chapters via pagination
- **API Efficiency**: 10x fewer requests (eliminated wasteful endpoints)
- **Target Performance**: <1 hour for full database download

## ðŸ› ï¸ Error Handling & Resilience

- **Smart Termination**: Stops after 3 consecutive 404s per chapter
- **Graceful Failure**: Failed downloads are logged but don't stop the scraper
- **Network Resilience**: Automatic timeout handling with fast recovery
- **Memory Efficient**: Streaming downloads prevent memory bloat
- **Server Friendly**: Intelligent rate limiting prevents server overload

## ðŸŽŠ Major Improvements

### v2.0 - Ultra Performance Update
- âœ… **25x faster page discovery** via parallel processing
- âœ… **2x more chapters discovered** via paginated chapter list support  
- âœ… **10x more efficient** by eliminating wasteful API calls
- âœ… **Smart termination** with 3 consecutive 404s logic
- âœ… **Complete parallel architecture** at every processing level
- âœ… **<1 hour target** for full database downloads

## âš–ï¸ Legal Notice

This tool is for educational purposes only. Please respect comick.live's terms of service and use responsibly. The scraper includes built-in rate limiting to be server-friendly.

## ðŸ” Troubleshooting

### Common Issues:

1. **"Permission denied" errors**: Make sure the scraper has write permissions in the current directory
2. **"Too many open files" error**: Reduce workers with `-workers=20`
3. **Server rate limiting**: The scraper auto-adjusts but you may need to reduce `-workers`
4. **Disk space**: Each manhwa can be several hundred MB to GB in size
5. **Network timeouts**: The scraper retries automatically with fast timeout recovery

### Finding Manhwa Slugs:

Visit comick.live and navigate to any manhwa. The slug is the last part of the URL:
- `https://comick.live/comic/tower-of-god` â†’ slug: `tower-of-god`
- `https://comick.live/comic/one-piece` â†’ slug: `one-piece`

### Performance Tuning:

- **For speed**: Use `-workers=30-50` with good internet
- **For stability**: Use `-workers=10-20` for slower connections  
- **For servers**: The scraper auto-manages rate limiting

## ðŸ¤ Contributing

Feel free to submit issues and pull requests to improve the scraper! The codebase is optimized for performance and maintainability.

---

**Built with â¤ï¸ for the manhwa community**
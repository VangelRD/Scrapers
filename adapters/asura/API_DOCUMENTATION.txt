Scraper for asurascans

Make a GO webscraper based on teh follwoing 

BASE URL IS ASURACOMIC.NET UNLESS STATED OTHERWISE

Starting to index all comicks we can use the /series?page=N where N is equal or lower than 20 since asura scans just stupid like that AND they strangly dont need cookies to make requests since they still work even without them like below

Request:

GET /series?page=1 HTTP/2
Host: asuracomic.net
Cache-Control: max-age=0
Sec-Ch-Ua: "Not=A?Brand";v="24", "Chromium";v="140"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Platform: "Linux"
Accept-Language: en-US,en;q=0.9
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Sec-Fetch-Site: same-origin
Sec-Fetch-Mode: navigate
Sec-Fetch-User: ?1
Sec-Fetch-Dest: document
Referer: https://asuracomic.net/
Accept-Encoding: gzip, deflate, br
Priority: u=0, iGET /series?page=1 HTTP/2


Response is contained in another file named series_response.txt because it big But from the esponse we will extarct all of these for eatch manhwa 

<a href="series/pick-me-up-infinite-gacha-54cc846f"> frrom the HTML where its split into the following key part's <a href="series/[SLUG]-[hid]"> IMPORTANT when parsing the HTML since it also include other a href's get only those that start with [ <a href="series/ ] since the rest are irellevant 

Now with the full [SLUG]-[hid]
we can do a GET on /series/[SLUG]-[hid]
like so and once again requiering no cookies 

GET /series/reaper-of-the-drifting-moon-4e28152d HTTP/2
Host: asuracomic.net
Sec-Ch-Ua: "Not=A?Brand";v="24", "Chromium";v="140"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Platform: "Linux"
Accept-Language: en-US,en;q=0.9
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Sec-Fetch-Site: same-origin
Sec-Fetch-Mode: navigate
Sec-Fetch-User: ?1
Sec-Fetch-Dest: document
Referer: https://asuracomic.net/
Accept-Encoding: gzip, deflate, br
Priority: u=0, i

This returns a ton of junk so full response can be found in full_resp_series.txt

From that we can take the descrription (optional) and the logo where we can filter out the junk since only the logo is /media/N/[random].webp since all the others are -optiomized or -thumbnail.webp or something similar where N is a random number

And if we want we can get the latest chapter but thats too much effor so no 

anyways Next up we do /series/[SLUG]-[hid]/chapter/N where N starts from 0 and gose up by 1 until we hit 3 404's in a row like so once again no cookies ???? : 

GET /series/reaper-of-the-drifting-moon-4e28152d/chapter/0 HTTP/2
Host: asuracomic.net
Cache-Control: max-age=0
Sec-Ch-Ua: "Not=A?Brand";v="24", "Chromium";v="140"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Platform: "Linux"
Accept-Language: en-US,en;q=0.9
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Sec-Fetch-Site: same-origin
Sec-Fetch-Mode: navigate
Sec-Fetch-User: ?1
Sec-Fetch-Dest: document
Accept-Encoding: gzip, deflate, br
Priority: u=0, i

response once again was too big full response is in Big_resp_chap.txt

from there we extarct the follwoing info the 
https://gg.asuracomic.net/storage/media/[randome]/conversions/0N-optimized.webp Thats found in the <script> tags and is easy to extarct since they are the only images that have a 0N-optimized.webp 
where N is the padge number 

Then to get the actuall images we do a Get on https://gg.asuracomic.net/storage/media/[randome]/conversions/0N-optimized.webp where we find random from the previouse and N gose up by 1 untill we get 3 404's in a row like so again no cookies neede : 

GET /storage/media/11896/conversions/01-optimized.webp HTTP/2
Host: gg.asuracomic.net
Sec-Ch-Ua: "Not=A?Brand";v="24", "Chromium";v="140"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Platform: "Linux"
Accept-Language: en-US,en;q=0.9
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Sec-Fetch-Site: none
Sec-Fetch-Mode: navigate
Sec-Fetch-User: ?1
Sec-Fetch-Dest: document
Accept-Encoding: gzip, deflate, br
Priority: u=0, i

This returns the image data for an example see image_data.txt

and after all that you have scraped 1 chapter :) 

Remember to keep the output format of your scraper like so 

downloads/
├── solo-leveling/
│   ├── cover.webp              ← Cover image
│   ├── chapter_1/
│   │   ├── 000.webp
│   │   ├── 001.webp
│   │   └── ...
│   ├── chapter_2/
│   │   ├── 000.webp
│   │   ├── 001.webp
│   │   └── ...
│   └── ...
├── tower-of-god/
│   ├── cover.jpg
│   ├── chapter_1/
│   └── ...
└── ...

and ahve a modualr file structure 